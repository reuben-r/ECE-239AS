{"cells":[{"cell_type":"markdown","metadata":{"id":"JtA1_PsYhs24"},"source":["## Setup\n","Similar to the previous projects, we will need some code to set up the environment.\n","\n","First, run this cell that loads the autoreload extension. This allows us to edit .py source files and re-import them into the notebook for a seamless editing and debugging experience.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OKXSEQjRh63r"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"eIXWSou6h_S6"},"source":["### Google Colab Setup\n","\n","Run the following cell to mount your Google Drive. Follow the link and sign in to your Google account (the same account you used to store this notebook!)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"evqEGDXRipC-"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"yLgYj1z49Ozn"},"source":["Then enter your path of the project (for example, /content/drive/MyDrive/ConditionalDDPM)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z8OU2pRkivyc"},"outputs":[],"source":["%cd your_project_path"]},{"cell_type":"markdown","metadata":{"id":"_nqWhiLojS8M"},"source":["We will use GPUs to accelerate our computation in this notebook. Go to `Runtime > Change runtime type` and set `Hardware accelerator` to `GPU`. This will reset Colab. **Rerun the top cell to mount your Drive again.** Run the following to make sure GPUs are enabled:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RdQhVgi5jVQp"},"outputs":[],"source":["# set the device\n","import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","if torch.cuda.is_available():\n","  print('Good to go!')\n","else:\n","  print('Please set GPU via the downward triangle in the top right corner.')"]},{"cell_type":"markdown","metadata":{"id":"6lSaZLNClxOM"},"source":["## Conditional Denoising Diffusion Probabilistic Models\n","\n","In the lectures, we have learnt about Denoising Diffusion Probabilistic Models (DDPM), as presented in the paper [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239.pdf). We went through both the training process and test sampling process of DDPM. In this project, you will use conditional DDPM to generate digits based on given conditions. The project is inspired by the paper [Classifier-free Diffusion Guidance](https://arxiv.org/pdf/2207.12598.pdf), which is a following work of DDPM. You are required to use MNIST dataset and the GPU device to complete the project.\n","\n","(It will take about 20\\~30 minutes (10 epochs) if you are using the free-version Google Colab GPU. Typically, realistic digits can be generated after around 2\\~5 epochs.)"]},{"cell_type":"markdown","metadata":{"id":"TCjduE1ZlxON"},"source":["### What is a DDPM?\n","\n","A Denoising Diffusion Probabilistic Model (DDPM) is a type of generative model inspired by the natural diffusion process. In the example of image generation, DDPM works in two main stages:\n","\n","- Forward Process (Diffusion): It starts with an image sampled from the dataset and gradually adds noise to it step by step, until it becomes completely random noise. In implementation, the forward diffusion process is fixed to a Markov chain that gradually adds Gaussian noise to the data according to a variance schedule $\\beta_1, ..., \\beta_T$.\n","\n","- Reverse Process (Denoising): By learning how the noise was added on the image step by step, the model can do the reverse process: start with random noise and step by step, remove this noise to generate an image."]},{"cell_type":"markdown","metadata":{"id":"rT7CWVDrlxON"},"source":["### Training and sampling of DDPM\n","\n","As proposed in the DDPM paper, the training and sampling process can be concluded in the following steps:\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1N5vswnDJDkM8QZ3ai8V7bfPi_fci3rwQ\" width=\"800\" height=\"200\">"]},{"cell_type":"markdown","metadata":{"id":"4DH1jyRWlxON"},"source":["Here we still use the example of image generation.\n","\n","Algorithm 1 shows the training process of DDPM. Initially, an image $\\textbf{x}_0$ is sampled from the data distribution $q(\\textbf{x}_0)$, i.e. the dataset. Then a time step $t$ is randomly selected from a uniform distribution across the predifined number of steps $T$.  \n","A noise $\\epsilon$ which has the same shape of the image is sampled from a standard normal distribution.\n","According to the equation (4) in the DDPM paper and the new notation: $q(\\textbf{x}_t|\\textbf{x}_0) = \\mathcal{N}(\\textbf{x}_t; \\sqrt{\\bar{\\alpha_t}}\\textbf{x}_0, (1 - \\bar{\\alpha_t})\\mathbf{I})$, $\\alpha_t := 1 - \\beta_t$ and $\\bar{\\alpha}_t := \\prod_{s=1}^{t} \\alpha_s$, we can get an intermediate state of the diffusion process: $\\textbf{x}_t = \\sqrt{\\bar{\\alpha_t}}\\textbf{x}_0 + \\sqrt{(1 - \\bar{\\alpha_t})}\\boldsymbol{\\epsilon}$.\n","The model takes the $\\textbf{x}_t$ and $t$ as inputs, and predict a noise, i.e. $\\epsilon_{\\theta}(\\sqrt{\\bar{\\alpha_t}}\\textbf{x}_0 + \\sqrt{(1 - \\bar{\\alpha_t})}\\boldsymbol{\\epsilon}, t)$.\n","The optimization of the model is done by minimize the difference between the sampled noise and the model's prediction of noise.\n","\n","Algorithm 2 shows the sampling process of DDPM, which is the complete procedure for generating an image. This process starts from noise $x_T$ sampled from a standard normal distribution, and then uses the trained model to iteratively apply denoising for each time step from $T$ to 1."]},{"cell_type":"markdown","metadata":{"id":"VxFgA0ZilxOO"},"source":["### How to control the generation output?\n","\n","As you may find, the vanilla DDPM can only randomly generate images which are sampled from the learned distribution of the dataset, while in some cases, we are more interested in controlling the content of generated images.\n","Previous works mainly use an extra trained classifier to guide the diffusion model to generate specific images ([Dhariwal & Nichol (2021)](https://arxiv.org/pdf/2105.05233.pdf)).\n","Ho et al. proposed the [Classifier-free Diffusion Guidance](https://arxiv.org/pdf/2207.12598.pdf), which proposes a novel training and sampling method to achieve the conditional generation without extra models besides the diffusion model.\n","Now let's see how it modify the training and sampling pipeline of DDPM."]},{"cell_type":"markdown","metadata":{"id":"E-cKqw-IlxOO"},"source":["##### Algorithm 1: Conditional training\n","The training process is shown in the picture below. Some notations are modified in order to follow DDPM.\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=134deA1V2M5KkmZlnNeX-x6REMyscmHXo\" alt=\"ConDDPM_1\" width=\"800\" height=\"240\"/>"]},{"cell_type":"markdown","metadata":{"id":"IzsH0iX8lxOO"},"source":["Compared with the training process of vanilla DDPM, there are several modifications.\n","\n","- In the training data sampling, besides the image $\\textbf{x}_0$, we also sample the condition $\\textbf{c}_0$ from the dataset (usually the class label).\n","\n","- There's a probabilistic step to randomly discard the conditions, training the model to generate data both conditionally and unconditionally. Usually we just set the one-hot encoded label as all -1 to discard the conditions.\n","\n","- When optimizing the model, the condition $\\textbf{c}_0$ is an extra input."]},{"cell_type":"markdown","metadata":{"id":"UJJ9oWWxlxOP"},"source":["##### Algorithm 2: Conditional sampling\n","\n","Below is the sampling process of conditional DDPM.\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1wL_CbuyG2jckpXb8oPjO-BLOEJZxNyk0\" width=\"500\" height=\"250\">\n","\n","<!-- ![ConDDPM_2](https://drive.google.com/uc?export=view&id=1wL_CbuyG2jckpXb8oPjO-BLOEJZxNyk0) -->"]},{"cell_type":"markdown","metadata":{"id":"Vv81HwB7lxOP"},"source":["Compared with the vanilla DDPM, the key modification is in step 4.\n","Here the algorithm computes a corrected noise estimation, $\\tilde{\\boldsymbol{\\epsilon}}_t$, balancing between the conditional prediction $\\boldsymbol{\\epsilon}_{\\theta}(\\textbf{x}_t, \\textbf{c}, t)$ and the unconditional prediction $\\boldsymbol{\\epsilon}_{\\theta}(\\textbf{x}_t, t)$. The corrected noise $\\tilde{\\boldsymbol{\\epsilon}}_t$ is then used to update $\\textbf{x}_t$ in step 5.\n","**Here we follow the setting of DDPM paper and define $\\sigma_t = \\sqrt{\\beta_t}$.**"]},{"cell_type":"markdown","metadata":{"id":"Cr-VmyeplxOP"},"source":["### Conditional generation of digits"]},{"cell_type":"markdown","metadata":{"id":"5hOc36A8lxOP"},"source":["Now let's practice it! You will first asked to design a denoising network, and then complete the training and sampling process of this conditional DDPM.\n","In this project, by default, we resize all images to a dimension of $28 \\times 28$ and utilize one-hot encoding for class labels."]},{"cell_type":"markdown","metadata":{"id":"Dxfsd2ZQlxOP"},"source":["First we define a configuration class `DMConfig`. This class contains all the settings of the model and experiment that may be useful later."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZoTsQ3B8lxOP"},"outputs":[],"source":["from dataclasses import dataclass, field\n","from typing import List, Tuple\n","@dataclass\n","class DMConfig:\n","    '''\n","    Define the model and experiment settings here\n","    '''\n","    input_dim: Tuple[int, int] = (28, 28) # input image size\n","    num_channels: int = 1                 # input image channels\n","    condition_mask_value: int = -1        # unconditional condition mask value\n","    num_classes: int = 10                 # number of classes in the dataset\n","    T: int = 400                          # diffusion and denoising steps\n","    beta_1: float = 1e-4                  # variance schedule\n","    beta_T: float = 2e-2\n","    mask_p: float = 0.1                   # unconditional condition drop ratio\n","    num_feat: int = 128                   # feature size of the UNet model\n","    omega: float = 2.0                    # conditional guidance weight\n","\n","    batch_size: int = 256                 # training batch size\n","    epochs: int = 10                      # training epochs\n","    learning_rate: float = 1e-4           # training learning rate\n","    multi_lr_milestones: List[int] = field(default_factory=lambda: [20]) # learning rate decay milestone\n","    multi_lr_gamma: float = 0.1           # learning rate decay ratio"]},{"cell_type":"markdown","metadata":{"id":"KEJd9oaJlxOP"},"source":["Then let's prepare and visualize the dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9XpuRAxslxOP"},"outputs":[],"source":["from utils import make_dataloader\n","from torchvision import transforms\n","import torchvision.utils as vutils\n","import matplotlib.pyplot as plt\n","\n","# Define the data preprocessing and configuration\n","transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.1307,), (0.3081,))\n","    ])\n","config = DMConfig()\n","\n","# Create the train and test dataloaders\n","train_loader = make_dataloader(transform = transform, batch_size = config.batch_size, dir = './data', train = True)\n","test_loader = make_dataloader(transform = transform, batch_size = config.batch_size, dir = './data', train = False)\n","\n","# Visualize the first 100 images\n","dataiter = iter(train_loader)\n","images, labels = next(dataiter)\n","images_subset = images[:100]\n","grid = vutils.make_grid(images_subset, nrow = 10, normalize = True, padding=2)\n","plt.figure(figsize=(6, 6))\n","plt.imshow(grid.numpy().transpose((1, 2, 0)))\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"HJ0njismlxOQ"},"source":["#### 1. Denoising network (4 points)\n","\n","The denoising network is defined in the file `ResUNet.py`. We have already provided some potentially useful blocks, and you will be asked to complete the class `ConditionalUnet`.\n","\n","Some hints:\n","\n","- **Please consider just using 2 down blocks and 2 up blocks. Using more blocks may improve the performance, while the training and sampling time may increase. Feel free to do some extra experiments in the creative exploring part later.**\n","\n","- **An example structure of Conditional UNet is shown in the next cell. Here the initialization argument `n_feat` is set as 128. We provide all the potential useful components in the `__init__` function. The simplest way to construct the network is to complete the `forward` function with these components**\n","\n","\n","- **You can design your own network and add any blocks. Feel free to modifiy or even remove the provided blocks or layers. You are also free to change the way of adding the time step and condition.**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pBpsUjQx1De4"},"outputs":[],"source":["# Example structure of Conditional UNet\n","from IPython.core.display import SVG\n","SVG(filename='./pics/ConUNet.svg')"]},{"cell_type":"markdown","metadata":{"id":"wNp2iBtZlxOQ"},"source":["Now let's check your denoising network using the following code."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PCfKq0lulxOQ"},"outputs":[],"source":["from ResUNet import ConditionalUnet\n","import torch\n","model = ConditionalUnet(in_channels = 1, n_feat = 128, n_classes = 10).to(device)\n","x = torch.randn((256,1,28,28)).to(device)\n","t = torch.randn((256,1,1,1)).to(device)\n","c = torch.randn((256,10)).to(device)\n","x_out = model(x,t,c)\n","assert x_out.shape == (256,1,28,28)\n","print('Output shape:', model(x,t,c).shape)\n","print('Dimension test passed!')"]},{"cell_type":"markdown","metadata":{"id":"YYc9ij6X2qBR"},"source":["**Before proceeding, please remember to normalize the time step $t$ to the range 0-1 before inputting it into the denoising network for the next part of the project. It will help the network have a more stable output.**"]},{"cell_type":"markdown","metadata":{"id":"if6RR_SKlxOQ"},"source":["#### 2. Conditional DDPM"]},{"cell_type":"markdown","metadata":{"id":"8ASkAX96lxOQ"},"source":["With the correct denoising network, we can then start to build the pipeline of a conditional DDPM.\n","You will be asked to complete the `ConditionalDDPM` class in the file `DDPM.py`."]},{"cell_type":"markdown","metadata":{"id":"5Rcl7cMVlxOQ"},"source":["##### 2.1 Variance schedule (3 points)\n","\n","Let's first prepare the variance schedule $\\beta_t$ along with other potentially useful constants.\n","You are required to complete the `ConditionalDDPM.scheduler` function in `DDPM.py`.\n","\n","Given the starting and ending variances $\\beta_1$ and $\\beta_T$, the function should output one dictionary containing the following terms:\n","\n","`beta_t`: variance of time step $t_s$, which is linearly interpolated between $\\beta_1$ and $\\beta_T$.\n","\n","`sqrt_beta_t`: $\\sqrt{\\beta_t}$\n","\n","`alpha_t`: $\\alpha_t = 1 - \\beta_t$\n","\n","`oneover_sqrt_alpha`: $\\frac{1}{\\sqrt{\\alpha_t}}$\n","\n","`alpha_t_bar`: $\\bar{\\alpha_t} = \\prod_{s=1}^{t} \\alpha_s$\n","\n","`sqrt_alpha_bar`: $\\sqrt{\\bar{\\alpha_t}}$\n","\n","`sqrt_oneminus_alpha_bar`: $\\sqrt{1 - \\bar{\\alpha_t}}$\n","\n","We set $\\beta_1 = 1e-4$ and $\\beta_T = 2e-2$. Let's check your solution!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BZLAzy17lxOQ"},"outputs":[],"source":["from DDPM import ConditionalDDPM\n","import torch\n","torch.set_printoptions(precision=8)\n","config = DMConfig(beta_1 = 1e-4, beta_T = 2e-2)\n","ConDDPM = ConditionalDDPM(dmconfig = config)\n","schedule_dict = ConDDPM.scheduler(t_s = torch.tensor(77)) # We use a specific time step (77) to check your output\n","assert torch.abs(schedule_dict['beta_t'] - 0.003890) <= 1e-5\n","assert torch.abs(schedule_dict['sqrt_beta_t'] - 0.062374) <= 1e-5\n","assert torch.abs(schedule_dict['alpha_t'] - 0.996110) <= 1e-5\n","assert torch.abs(schedule_dict['oneover_sqrt_alpha'] - 1.001951) <= 1e-5\n","assert torch.abs(schedule_dict['alpha_t_bar'] - 0.857414) <= 1e-5\n","assert torch.abs(schedule_dict['sqrt_oneminus_alpha_bar'] - 0.377606) <= 1e-5\n","print('All tests passed!')"]},{"cell_type":"markdown","metadata":{"id":"p2dqrKfflxOQ"},"source":["##### 2.2 Training process (5 points)\n","\n","Recall the training algorithm we discussed above:\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=134deA1V2M5KkmZlnNeX-x6REMyscmHXo\" alt=\"ConDDPM_1\" width=\"800\" height=\"240\"/>\n","\n","You will need to complete the  `ConditionalDDPM.forward` function in the `DDPM.py` file. Then you can use the function `utils.check_forward` to test if it's working properly. The model will be trained for one epoch in this checking process. It should take around 2 min and return one curve showing a decreasing loss trend if your `ConditionalDDPM.forward` function is correct."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ux8I36mClxOQ"},"outputs":[],"source":["from utils import check_forward\n","config = DMConfig()\n","model = check_forward(train_loader, config, device)"]},{"cell_type":"markdown","metadata":{"id":"HUivN4j7lxOQ"},"source":["2.3 Sampling process (5 points)\n","\n","Now you are required to complete the `ConditionalDDPM.sample` function using the sampling process we mentioned above.\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1wL_CbuyG2jckpXb8oPjO-BLOEJZxNyk0\" width=\"500\" height=\"250\">\n","\n","In the following cell, we will use the given `utils.check_sample` function to check the correctness. With the trained model in 2.2, the model should be able to generate some super-rough digits (you may not even see them as digits). The sampling process should take about 1 minute."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D0fi_JI1lxOQ"},"outputs":[],"source":["from utils import check_sample\n","config = DMConfig()\n","fig = check_sample(model, config, device)"]},{"cell_type":"markdown","metadata":{"id":"gCHrx4KYlxOQ"},"source":["##### 2.4 Full training (5 points)\n","\n","As you might notice, the images generated are imperfect since the model trained for only one epoch has not yet converged. To improve the model's performance, we should proceed with a complete cycle of training and testing. You can utilize the provided `solver` function in this part."]},{"cell_type":"markdown","metadata":{"id":"FP08oE8plxOQ"},"source":["Let's recall all model and experiment configurations:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ltohmzZwlxOQ"},"outputs":[],"source":["train_config = DMConfig()\n","print(train_config)"]},{"cell_type":"markdown","metadata":{"id":"eHvJBOlTlxOR"},"source":["Then we can use function `utils.solver` to train the model. You should also input your own experiment name, e.g. `your_exp_name`. The best-trained model will be saved as `./save/your_exp_name/best_checkpoint.pth`. Furthermore, for each training epoch, one generated image will be stored in the directory `./save/your_exp_name/images`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rgvzKBhxlxOR"},"outputs":[],"source":["from utils import solver\n","solver(dmconfig = train_config,\n","       exp_name = 'your_exp_name',\n","       train_loader = train_loader,\n","       test_loader = test_loader)"]},{"cell_type":"markdown","metadata":{"id":"Fbzlbw3glxOR"},"source":["**Now please show the image that you believe has the best generation quality in the following cell.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dkW-H-x5lxOR"},"outputs":[],"source":["# ==================================================== #\n","# YOUR CODE HERE:\n","#   Among all images generated in the experiment,\n","#   show the image that you believe has the best generation quality.\n","#   You may use tools like matplotlib, PIL, OpenCV, ...\n","\n","\n","\n","\n","\n","# ==================================================== #"]},{"cell_type":"markdown","metadata":{"id":"Mu8dT5LLlxOR"},"source":["##### 2.5 Exploring the conditional guidance weight (3 points)\n","\n","The generated images from the previous training-sampling process is using the default conditional guidance weight $\\omega=2$. Now with the best checkpoint, please try at least 3 different $\\omega$ values and visualize the generated images. You can use the provided function `sample_images` to get a combined image each time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bByESJhllxOR"},"outputs":[],"source":["from utils import sample_images\n","import matplotlib.pyplot as plt\n","# ==================================================== #\n","# YOUR CODE HERE:\n","#   Try at least 3 different conditional guidance weights and visualize it.\n","#   Example of using a different omega value:\n","#       sample_config = DMConfig(omega = ?)\n","#       fig = sample_images(config = sample_config, checkpoint_path = path_to_your_checkpoint)\n","\n","\n","# ==================================================== #"]},{"cell_type":"markdown","metadata":{"id":"hUYaOF1slxOR"},"source":["**Inline Question: Based on your experiment, discuss how the conditional guidance weight affects the quality and diversity of generation.**"]},{"cell_type":"markdown","metadata":{"id":"_-a7vm9ElxOR"},"source":["Your answer:\n"]},{"cell_type":"markdown","metadata":{"id":"WYn7hhzClxOV"},"source":["##### 2.6 Customize your own model (5 points)\n","\n","Now let's experiment by modifying some hyperparameters in the config and costomizing your own model. You should at least change one defalut setting in the config and train a new model. Then visualize the generation image and discuss the effects of your modifications.\n","\n","**Hint: Possible changes to the configuration include, but are not limited to,  the number of diffusion steps $T$, the unconditional condition drop ratio $mask\\_p$, the feature size $num\\_feat$, the beta schedule, etc.**"]},{"cell_type":"markdown","metadata":{"id":"IIjyHpK8lxOV"},"source":["First you should define and print your modified config. Please state all the changes you made to the DMConfig class, i.e. `DMConfig(T=?, num_feat=?, ...)`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TaavtHN4lxOW"},"outputs":[],"source":["# ==================================================== #\n","# YOUR CODE HERE:\n","#   Your new configuration:\n","#   train_config_new = DMConfig(...)\n","\n","\n","# ==================================================== #\n","print(train_config_new)"]},{"cell_type":"markdown","metadata":{"id":"CK2q7ayMlxOW"},"source":["Then similar to 2.4, use `solver` funtion to complete the training and sampling process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XRhdh6XQlxOW"},"outputs":[],"source":["from utils import solver\n","solver(dmconfig = train_config_new,\n","       exp_name = 'your_exp_name_new',\n","       train_loader = train_loader,\n","       test_loader = test_loader)"]},{"cell_type":"markdown","metadata":{"id":"VNf1UIiGlxOW"},"source":["Finally, show one image that you think has the best quality."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xH7S__QKlxOW"},"outputs":[],"source":["# ==================================================== #\n","# YOUR CODE HERE:\n","#   Among all images generated in the experiment,\n","#   show the image that you believe has the best generation quality.\n","#   You may use tools like matplotlib, PIL, OpenCV, ...\n","\n","\n","\n","\n","# ==================================================== #"]},{"cell_type":"markdown","metadata":{"id":"P9ulLpLdlxOW"},"source":["**Inline Question: Discuss the effects of your modifications after you compare the generation performance under different configurations.**"]},{"cell_type":"markdown","metadata":{"id":"8DcNnuDvlxOW"},"source":["Your answer:"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}